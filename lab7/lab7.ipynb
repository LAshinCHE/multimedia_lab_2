{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c2c7f8fe-563b-428b-80d2-1876ec0e3717","cell_type":"markdown","source":"## Черевичин Егор М8О-406Б-21 Лабораторная работа 7","metadata":{}},{"id":"c1fdbe11-c410-40c5-977f-de629b6c8864","cell_type":"code","source":"import os\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport segmentation_models_pytorch as smp\nfrom torchvision.datasets import VOCSegmentation\n\nfrom torchmetrics.classification import MulticlassJaccardIndex\n\nSEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nNUM_CLASSES = 21","metadata":{"tags":[]},"outputs":[],"execution_count":1},{"id":"0a2c1905","cell_type":"markdown","source":"# Обоснования выбора VOC2012 \n1. Общеизвестный бенчмарк\nVOC2012 — классический датасет, широко используемый в задачах семантической сегментации. Он позволяет объективно сравнивать модели с результатами других работ.\n\n2. Разнообразие классов\nВключает 20 классов объектов и фон — охватывает широкий спектр категорий (животные, транспорт, люди и т.д.), что делает его универсальным для оценки сегментаторов.\n\n3. Качественная разметка\nВсе маски размечены вручную, обеспечивая точные и надёжные аннотации.\n\n4. Умеренная сложность\nСцены содержат частично перекрывающиеся объекты и разнообразные формы, что делает датасет достаточно сложным для обучения и тестирования моделей.\n\n5. Поддержка в популярных фреймворках\nVOC2012 встроен в PyTorch, TensorFlow и другие библиотеки, что упрощает его использование и интеграцию в пайплайны.","metadata":{}},{"id":"d765d765","cell_type":"markdown","source":"### Подготовка датасета\n\n#### Аугментации (Albumentations)\n\n| Режим       | Описание                                                                                      |\n|-------------|-----------------------------------------------------------------------------------------------|\n| **train**   | - Случайный масштаб и кроп до 512×512  <br> - Горизонтальный флип  <br> - Color jitter        |\n|             | - Нормализация по статистике ImageNet  <br> - Преобразование в тензор                         |\n| **val**     | - Масштаб по длинной стороне до 512 px  <br> - Паддинг до 512×512                             |\n|             | - Нормализация как в train  <br> - Преобразование в тензор                                    |\n\n#### Класс `VOCDataset`\n\n| Компонент         | Описание                                                                                  |\n|-------------------|-------------------------------------------------------------------------------------------|\n| Источник          | `torchvision.datasets.VOCSegmentation(year=2012)`                                         |\n| `__getitem__`     | - Конвертация изображений и масок из PIL в NumPy <br> - Совместные аугментации пары      |\n|                   | - Возврат: изображение (`float`), маска (`long`)                                          |\n\n#### DataLoader\n\n| Назначение  | batch_size | shuffle | pin_memory | num_workers | Комментарий                              |\n|-------------|------------|---------|------------|-------------|------------------------------------------|\n| **train**   | 8          | True    | True       | 4           | Быстрая загрузка и случайное перемешивание |\n| **val**     | 8          | False   | True       | 4           | Детерминированная оценка                  |\n\n","metadata":{}},{"id":"68b39045-6fcb-4ee7-91ee-7a093ba4bb96","cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import VOCSegmentation\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ntrain_tfms = A.Compose([\n    A.RandomResizedCrop(size=(512, 512), scale=(0.5, 1.0), ratio=(0.75, 1.33), p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.ColorJitter(0.2, 0.2, 0.2, 0.1, p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406),\n                std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_tfms = A.Compose([\n    A.LongestMaxSize(512),\n    A.PadIfNeeded(512, 512),\n    A.Normalize(mean=(0.485, 0.456, 0.406),\n                std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nclass VOCDataset(torch.utils.data.Dataset):\n    def __init__(self, root, image_set, tfms):\n        self.voc = VOCSegmentation(\n            root=root,\n            year=\"2012\",\n            image_set=image_set,\n            download=True,\n        )\n        self.tfms = tfms\n\n    def __len__(self):\n        return len(self.voc)\n\n    def __getitem__(self, idx):\n        img, mask = self.voc[idx]\n        aug = self.tfms(image=np.array(img), mask=np.array(mask))\n        image = aug[\"image\"]\n        mask = torch.as_tensor(aug[\"mask\"], dtype=torch.long)\n        return image, mask\n\nds_train = VOCDataset(\n    root=\"./data\",\n    image_set=\"train\",\n    tfms=train_tfms,\n)\nds_val = VOCDataset(\n    root=\"./data\",\n    image_set=\"val\",\n    tfms=val_tfms,\n)\n\ndl_train = DataLoader(\n    ds_train,\n    batch_size=8,\n    shuffle=True,\n    pin_memory=True,\n    num_workers=4,\n)\ndl_val = DataLoader(\n    ds_val,\n    batch_size=8,\n    shuffle=False,\n    pin_memory=True,\n    num_workers=4,\n)","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./data/VOCtrainval_11-May-2012.tar\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1999639040/1999639040 [00:36<00:00, 55104474.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n","Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n","Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"]}],"execution_count":null},{"id":"f94581f1-efff-4cd8-8b97-f0da208d7c8b","cell_type":"markdown","source":"## Метрики и общий цикл обучения","metadata":{}},{"id":"f973515d","cell_type":"markdown","source":"### Функции потерь и метрики\n\n| Название                         | Описание                                                                                   | Особенности                                           |\n|----------------------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------|\n| **CrossEntropy + DiceLoss**      | Комбинация двух функций потерь:                                                            | 70% CrossEntropy + 30% DiceLoss                       |\n|                                  | - CrossEntropy для точной пиксельной классификации                                         | Игнорируются пиксели с меткой `255` (нет разметки)   |\n|                                  | - Dice Loss для улучшения перекрытия предсказанных и истинных масок                       |                                                      |\n| **IoU (Intersection over Union)**| Оценка качества сегментации — отношение площади пересечения к объединению сегментов       | Вычисляется по каждому классу                         |\n| **mIoU (mean IoU)**              | Среднее значение IoU по всем классам                                                       | Ключевая метрика, исключает метку `255`              |\n| **Dice Loss**                    | Измеряет степень перекрытия, чувствителен к малым сегментам                                | Часто используется в дополнение к CrossEntropy        |\n\n---\n\n#### Визуальное сравнение IoU и Dice (схематично)\n\n- **IoU** = |A ∩ B| / |A ∪ B|  \n- **Dice** = 2 × |A ∩ B| / (|A| + |B|)\n\nГде:\n- A — предсказанная маска  \n- B — истинная маска\n\n> Dice более чувствителен к небольшим объектам и может лучше работать при несбалансированных классах.\n\n","metadata":{}},{"id":"f5e4847a-8aec-4392-a714-8bbc470f6fbf","cell_type":"code","source":"IGNORE = 255\n\ndef get_metrics():\n    return MulticlassJaccardIndex(\n        num_classes=NUM_CLASSES,\n        ignore_index=IGNORE,\n    ).to(DEVICE)\n\ndice_loss = smp.losses.DiceLoss(\n    mode=\"multiclass\",\n    ignore_index=IGNORE,\n    smooth=1e-5,\n)\nce_loss = nn.CrossEntropyLoss(ignore_index=IGNORE)\n\ndef loss_fn(logits, targets):\n    return 0.7 * ce_loss(logits, targets) + 0.3 * dice_loss(logits, targets)\n\ndef train_one_epoch(model, opt, scaler):\n    model.train()\n    for x, y in dl_train:\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        opt.zero_grad()\n        with torch.cuda.amp.autocast():\n            loss = loss_fn(model(x), y)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n\n@torch.no_grad()\ndef evaluate(model, metric):\n    model.eval(); metric.reset()\n    for x, y in dl_val:\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        logits = model(x).argmax(1)\n        metric.update(preds=logits, target=y)\n    return metric.compute().item()\n\ndef run_training(model, epochs=10, lr=1e-3):\n    model.to(DEVICE)\n    opt = torch.optim.AdamW(\n        model.parameters(),\n        lr=lr,\n        weight_decay=1e-2,\n    )\n    sch = torch.optim.lr_scheduler.LambdaLR(\n        opt,\n        lambda e: (1 - e / epochs) ** 0.9,\n    )\n    scaler = torch.cuda.amp.GradScaler()\n    metric = get_metrics()\n\n    for ep in range(epochs):\n        train_one_epoch(model, opt, scaler)\n        miou = evaluate(model, metric)\n        print(f\"E{ep:02d}: mIoU={miou:.3f}\")\n        sch.step()\n\n    return miou","metadata":{"tags":[]},"outputs":[],"execution_count":3},{"id":"5cb13dae-f9f5-43a8-b18c-4bf4dcb6dc0b","cell_type":"markdown","source":"## Baseline модели","metadata":{"tags":[]}},{"id":"1f6a57e6-3b09-4492-be3a-7467303fe553","cell_type":"markdown","source":"### Обычная ResNet","metadata":{"tags":[]}},{"id":"00673760","cell_type":"markdown","source":"Baseline ResNet34-U-Net\n\n- **Архитектура**: U-Net с энкодером ResNet34 (предобучен на ImageNet), 21 класс сегментации.  \n- **Тренинг**: 20 эпох, lr=1e-3, оптимизатор AdamW, смешанная точность, LambdaLR-шедулер.  ","metadata":{}},{"id":"bddd00b1-6b8d-4dca-a205-345fd6cc3852","cell_type":"code","source":"baseline_cnn = smp.Unet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    classes=NUM_CLASSES,\n).to(DEVICE)\n\nmiou_base_cnn = run_training(\n    model=baseline_cnn,\n    epochs=20,\n    lr=1e-3,\n)\n\nprint(f\"Baseline-CNN mIoU: {miou_base_cnn:.3f}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["E00: mIoU=0.039\n","E01: mIoU=0.043\n","E02: mIoU=0.047\n","E03: mIoU=0.057\n","E04: mIoU=0.079\n","E05: mIoU=0.071\n","E06: mIoU=0.073\n","E07: mIoU=0.082\n","E08: mIoU=0.099\n","E09: mIoU=0.085\n","E10: mIoU=0.095\n","E11: mIoU=0.108\n","E12: mIoU=0.086\n","E13: mIoU=0.104\n","E14: mIoU=0.115\n","E15: mIoU=0.117\n","E16: mIoU=0.127\n","E17: mIoU=0.120\n","E18: mIoU=0.124\n","E19: mIoU=0.134\n","Baseline-CNN mIoU: 0.134\n"]}],"execution_count":12},{"id":"ddd5d7d6-ae55-4383-bd31-bb5575ac84b6","cell_type":"markdown","source":"### Transformer","metadata":{}},{"id":"3a6893eb","cell_type":"markdown","source":"### Обучение модели\n\n- **Модель:**  \n  Используется архитектура SegFormer с энкодером **MiT-B0**, предварительно обученным на ImageNet.\n\n- **Процесс обучения:**  \n  Запуск функции: `run_training(model, epochs=20, lr=1e-4)`\n\n  Параметры:\n  - Количество эпох: **20**\n  - Начальная скорость обучения: **1e-4**\n  - Оптимизатор: **AdamW**\n  - Планировщик обучения: **экспоненциальный LR scheduler**\n  - Активирован **AMP (смешанная точность)** для ускорения и экономии памяти\n  - Основная метрика: **mIoU** (mean Intersection over Union) на валидационном наборе\n","metadata":{}},{"id":"b4ce9c78-ad9a-44c7-87a6-94aa323f7829","cell_type":"code","source":"baseline_trans = smp.Segformer(\n    encoder_name=\"mit_b0\",\n    encoder_weights=\"imagenet\",\n    classes=NUM_CLASSES,\n).to(DEVICE)\n\nmiou_base_trans = run_training(\n    model=baseline_trans,\n    epochs=20,\n    lr=1e-4,\n)\n\nprint(f\"Baseline-Transformer mIoU: {miou_base_trans:.3f}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["E00: mIoU=0.404\n","E01: mIoU=0.518\n","E02: mIoU=0.535\n","E03: mIoU=0.566\n","E04: mIoU=0.579\n","E05: mIoU=0.569\n","E06: mIoU=0.598\n","E07: mIoU=0.579\n","E08: mIoU=0.613\n","E09: mIoU=0.626\n","E10: mIoU=0.600\n","E11: mIoU=0.624\n","E12: mIoU=0.636\n","E13: mIoU=0.637\n","E14: mIoU=0.632\n","E15: mIoU=0.643\n","E16: mIoU=0.656\n","E17: mIoU=0.653\n","E18: mIoU=0.650\n","E19: mIoU=0.650\n","Baseline-Transformer mIoU: 0.650\n"]}],"execution_count":13},{"id":"d404e6a2","cell_type":"markdown","source":"### Динамика обучения\n\n- **Начальная фаза (эпохи 0–3):**  \n  Резкий рост mIoU — от 0.404 до 0.566. Модель быстро учится выделять основные объекты на изображениях.\n\n- **Средняя фаза (эпохи 4–11):**  \n  Прирост становится более умеренным: mIoU повышается с 0.579 до 0.624. Видны небольшие колебания, но тренд остаётся положительным.\n\n- **Поздняя фаза (эпохи 12–19):**  \n  Модель выходит на плато — метрика достигает 0.650. Улучшения становятся минимальными, что указывает на стабилизацию обучения.\n\n---\n\n- **Итоговая mIoU после 20 эпох:** `0.650`  \n- **Общее поведение:**  \n  Значительное улучшение наблюдается уже в первые 3–4 эпохи, далее рост замедляется и постепенно выходит на насыщение.\n\n> Архитектура SegFormer (MiT-B0) заметно превосходит U-Net с ResNet34 — достигает более высокой точности сегментации и быстрее сходится при обучении.\n","metadata":{}},{"id":"37d60f81-6e22-4a0e-be70-9c8c2a947c28","cell_type":"markdown","source":"## Улучшенные бейзлайны","metadata":{}},{"id":"65bb5a32-c941-4b1a-a274-abf686b2b644","cell_type":"markdown","source":"### Улучшенный бейзлайн обычной","metadata":{}},{"id":"b736b58d","cell_type":"markdown","source":"**Архитектура:** U-Net++ с энкодером EfficientNet-B5 (ImageNet) и SCSE-вниманием в декодере.  \n**Данные:** VOC2012 (train/val).  \n**Тренинг:** epochs=20, lr=5·10⁻⁴, AdamW, комбинированный loss (CE + Dice).","metadata":{}},{"id":"2b32cc3f-3bc9-44f5-91ca-baae330ab2f4","cell_type":"code","source":"imp_cnn = smp.UnetPlusPlus(\n    encoder_name=\"tu-efficientnet_b5\",\n    encoder_weights=\"imagenet\",\n    classes=NUM_CLASSES,\n    decoder_attention_type=\"scse\",\n).to(DEVICE)\n\nmiou_imp_cnn = run_training(\n    model=imp_cnn,\n    epochs=10,\n    lr=5e-4,\n)\n\nprint(f\"Improved-CNN mIoU: {miou_imp_cnn:.3f}\")","metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"]},{"name":"stdout","output_type":"stream","text":["E00: mIoU=0.053\n","E01: mIoU=0.075\n","E02: mIoU=0.086\n","E03: mIoU=0.097\n","E04: mIoU=0.118\n","E05: mIoU=0.139\n","E06: mIoU=0.150\n","E07: mIoU=0.176\n","E08: mIoU=0.156\n","E09: mIoU=0.181\n","E10: mIoU=0.184\n","E11: mIoU=0.168\n","E12: mIoU=0.160\n","E13: mIoU=0.216\n","E14: mIoU=0.207\n","E15: mIoU=0.239\n","E16: mIoU=0.251\n","E17: mIoU=0.264\n","E18: mIoU=0.266\n","E19: mIoU=0.257\n","Improved-CNN mIoU: 0.257\n"]}],"execution_count":6},{"id":"069b934f-e1e0-4456-911e-06b12e96fd06","cell_type":"markdown","source":"### Улучшенный трансформер","metadata":{}},{"id":"94c883fe","cell_type":"markdown","source":"### Обучение улучшенной трансформерной модели\n\n- **Модель:**  \n  Используется SegFormer с энкодером **MiT-B2** — более глубокая и мощная версия по сравнению с MiT-B0.  \n  Настройка декодера: `decoder_segmentation_channels=512`.  \n  Весы энкодера и декодера инициализированы из модели, предобученной на ImageNet.\n\n- **Процесс обучения:**  \n  Вызов: `run_training(model=imp_trans, epochs=20, lr=1.5e-4)`\n\n  Параметры обучения:\n  - Эпохи: **20**\n  - Начальное значение learning rate: **1.5e-4**\n  - Оптимизатор: **AdamW**\n  - Планировщик: **LambdaLR** — позволяет гибко управлять спадом lr\n  - Используется **смешанная точность (AMP)** для ускорения и экономии памяти\n  - Ключевая метрика: **mIoU** (среднее IoU по классам) на валидационном наборе\n","metadata":{}},{"id":"732f5bc8-6bbc-4d28-b7a3-8ef10cd60307","cell_type":"code","source":"imp_trans = smp.Segformer(\n    encoder_name=\"mit_b2\",\n    encoder_weights=\"imagenet\",\n    classes=NUM_CLASSES,\n    decoder_segmentation_channels=512,\n).to(DEVICE)\n\nmiou_imp_trans = run_training(\n    model=imp_trans,\n    epochs=20,\n    lr=1.5e-4,\n)\n\nprint(f\"Improved-Transformer mIoU: {miou_imp_trans:.3f}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["E00: mIoU=0.263\n","E01: mIoU=0.374\n","E02: mIoU=0.419\n","E03: mIoU=0.353\n","E04: mIoU=0.428\n","E05: mIoU=0.491\n","E06: mIoU=0.485\n","E07: mIoU=0.563\n","E08: mIoU=0.541\n","E09: mIoU=0.585\n","E10: mIoU=0.596\n","E11: mIoU=0.581\n","E12: mIoU=0.632\n","E13: mIoU=0.578\n","E14: mIoU=0.636\n","E15: mIoU=0.645\n","E16: mIoU=0.671\n","E17: mIoU=0.672\n","E18: mIoU=0.674\n","E19: mIoU=0.684\n","Improved-Transformer mIoU: 0.684\n"]}],"execution_count":7},{"id":"e8b06921","cell_type":"markdown","source":"### Результаты обучения\n\n- **Стартовая точка:**  \n  Обучение началось с mIoU = 0.263 на первой эпохе (E00).\n\n- **Лучший результат:**  \n  К 19-й эпохе (E19) модель достигла максимального значения mIoU = 0.684.\n\n- **Динамика роста:**  \n  Существенный прирост начался после 6-й эпохи, затем наблюдался стабильный прогресс вплоть до завершения обучения.  \n  Финальное значение mIoU (0.684) превышает результат базовой версии трансформера (0.650).\n\n- **Вывод:**  \n  Более глубокий энкодер **MiT-B2** позволяет модели эффективнее захватывать контекст и пространственные зависимости, что напрямую отражается на улучшении качества сегментации по сравнению с MiT-B0.\n","metadata":{}},{"id":"751cb6da-f08f-4b0d-b7bf-2cf9f3961d8d","cell_type":"markdown","source":"## Свои модели","metadata":{}},{"id":"54e53ecc-9264-4ad8-908f-e83c569b2838","cell_type":"markdown","source":"### Обычная","metadata":{"tags":[]}},{"id":"c44472df","cell_type":"markdown","source":"### Обучение собственной CNN-модели\n\n- **Архитектура:**  \n  Лёгкий вариант U-Net с четырьмя уровнями энкодера и декодера.  \n  Начальная ширина каналов — **32**. Каждый блок состоит из последовательности: **Conv(3×3) → BatchNorm → ReLU**.  \n  Для апсемплинга используется **ConvTranspose2d**.\n\n- **Обучение:**  \n  Модель обучалась **12 эпох** с начальными параметрами:  \n  - Learning rate: **1e-3**\n","metadata":{}},{"id":"c43c4bce-b2de-4a8c-95cb-38809a048060","cell_type":"code","source":"def CBR(in_c: int, out_c: int) -> nn.Sequential:\n    return nn.Sequential(\n        nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, bias=False),\n        nn.BatchNorm2d(out_c),\n        nn.ReLU(inplace=True),\n    )\n\n\nclass Down(nn.Module):\n    def __init__(self, in_c: int, out_c: int):\n        super().__init__()\n        self.seq = nn.Sequential(\n            CBR(in_c, out_c),\n            CBR(out_c, out_c),\n        )\n\n    def forward(self, x):\n        return self.seq(x)\n\n\nclass Up(nn.Module):\n    def __init__(self, in_c: int, out_c: int):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(\n            in_c,\n            in_c // 2,\n            kernel_size=2,\n            stride=2,\n        )\n        self.conv = nn.Sequential(\n            CBR(in_c, out_c),\n            CBR(out_c, out_c),\n        )\n\n    def forward(self, x, skip):\n        x = self.up(x)\n        x = torch.cat([x, skip], dim=1)\n        return self.conv(x)\n\n\nclass UNetLite(nn.Module):\n    def __init__(self, n_cls: int = NUM_CLASSES, base: int = 32):\n        super().__init__()\n        self.d1 = Down(3, base)\n        self.d2 = Down(base, base * 2)\n        self.d3 = Down(base * 2, base * 4)\n        self.d4 = Down(base * 4, base * 8)\n        self.pool = nn.MaxPool2d(2)\n        self.bridge = Down(base * 8, base * 16)\n\n        self.u4 = Up(base * 16, base * 8)\n        self.u3 = Up(base * 8, base * 4)\n        self.u2 = Up(base * 4, base * 2)\n        self.u1 = Up(base * 2, base)\n\n        self.out_conv = nn.Conv2d(base, n_cls, kernel_size=1)\n\n    def forward(self, x):\n        x1 = self.d1(x)\n        x2 = self.d2(self.pool(x1))\n        x3 = self.d3(self.pool(x2))\n        x4 = self.d4(self.pool(x3))\n        x = self.bridge(self.pool(x4))\n        x = self.u4(x, x4)\n        x = self.u3(x, x3)\n        x = self.u2(x, x2)\n        x = self.u1(x, x1)\n        return self.out_conv(x)\n\n\nmy_cnn = UNetLite().to(DEVICE)\n\nmiou_my_cnn = run_training(\n    model=my_cnn,\n    epochs=12,\n    lr=1e-3,\n)\n\nprint(f\"UNet-Lite mIoU: {miou_my_cnn:.3f}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["E00: mIoU=0.039\n","E01: mIoU=0.039\n","E02: mIoU=0.040\n","E03: mIoU=0.043\n","E04: mIoU=0.045\n","E05: mIoU=0.045\n","E06: mIoU=0.045\n","E07: mIoU=0.043\n","E08: mIoU=0.044\n","E09: mIoU=0.042\n","E10: mIoU=0.044\n","E11: mIoU=0.049\n","E12: mIoU=0.051\n","E13: mIoU=0.052\n","E14: mIoU=0.052\n","UNet-Lite mIoU: 0.052\n"]}],"execution_count":8},{"id":"820249e8","cell_type":"markdown","source":"### Краткие выводы\n\n- **Ограниченная выразительность:**  \n  Модель без предобучения и с малым количеством параметров не справляется с задачей сложной сегментации.\n\n- **Медленное обучение:**  \n  Прирост mIoU идёт очень слабо, итоговое качество остаётся низким даже после нескольких эпох.\n\n- **Рекомендации по улучшению:**  \n  Для прогресса стоит увеличить архитектурную ёмкость — добавить больше уровней или каналов, использовать предобученные энкодеры (например, из Unet++ или SegFormer), а также применить более агрессивные аугментации.\n","metadata":{}},{"id":"4aa9d0b4-208a-4997-b9c3-b346715ec7d2","cell_type":"markdown","source":"### Улучшенная","metadata":{}},{"id":"5d2ad612","cell_type":"markdown","source":"- **Архитектура**:  \n  UNet-Lite + SCSE (Channel & Spatial Squeeze-Excitation) после каждого Up-блока, базовая ширина канала = 48.  \n- **Обучение**:  \n  20 эпох, lr = 5 × 10⁻⁴, AdamW + LambdaLR, AMP, комбинированный loss = 0.7 × CE + 0.3 × Dice, метрика – mIoU.","metadata":{}},{"id":"00b3e332-6c17-4426-8b42-cd0d6b1f4111","cell_type":"code","source":"class SCSE(nn.Module):\n    def __init__(self, c: int, r: int = 16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(c, c // r, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(c // r, c, kernel_size=1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(\n            nn.Conv2d(c, 1, kernel_size=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\n\nclass UNetLiteSCSE(UNetLite):\n    def __init__(self, n_cls: int = NUM_CLASSES, base: int = 48):\n        super().__init__(n_cls, base)\n        self.scse4 = SCSE(base * 8)\n        self.scse3 = SCSE(base * 4)\n        self.scse2 = SCSE(base * 2)\n        self.scse1 = SCSE(base)\n\n    def forward(self, x):\n        x1 = self.d1(x)\n        x2 = self.d2(self.pool(x1))\n        x3 = self.d3(self.pool(x2))\n        x4 = self.d4(self.pool(x3))\n        x = self.bridge(self.pool(x4))\n        x = self.scse4(self.u4(x, x4))\n        x = self.scse3(self.u3(x, x3))\n        x = self.scse2(self.u2(x, x2))\n        x = self.scse1(self.u1(x, x1))\n        return self.out_conv(x)\n\n\nmy_cnn_imp = UNetLiteSCSE().to(DEVICE)\n\nmiou_my_cnn_imp = run_training(\n    model=my_cnn_imp,\n    epochs=15,\n    lr=5e-4,\n)\n\nprint(f\"Improved-UNet-Lite mIoU: {miou_my_cnn_imp:.3f}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["E00: mIoU=0.039\n","E01: mIoU=0.039\n","E02: mIoU=0.039\n","E03: mIoU=0.039\n","E04: mIoU=0.039\n","E05: mIoU=0.041\n","E06: mIoU=0.042\n","E07: mIoU=0.040\n","E08: mIoU=0.041\n","E09: mIoU=0.042\n","E10: mIoU=0.041\n","E11: mIoU=0.043\n","E12: mIoU=0.046\n","E13: mIoU=0.046\n","E14: mIoU=0.049\n","E15: mIoU=0.050\n","E16: mIoU=0.051\n","E17: mIoU=0.050\n","E18: mIoU=0.051\n","E19: mIoU=0.054\n","Improved-UNet-Lite mIoU: 0.054\n"]}],"execution_count":9},{"id":"72078a40-52a9-44d5-b944-d7ed6b66fa19","cell_type":"markdown","source":"## Результаты","metadata":{}},{"id":"75b1684a-9e1a-4331-9142-2390d8428cfb","cell_type":"code","source":"import pandas as pd\n\nresults = pd.DataFrame(\n    {\n        \"Model\": [\n            \"Baseline-CNN\",\n            \"Baseline-Trans\",\n            \"Improved-CNN\",\n            \"Improved-Trans\",\n            \"My-CNN\",\n            \"My-CNN-Imp\",\n        ],\n        \"mIoU\": [\n            miou_base_cnn,\n            miou_base_trans,\n            miou_imp_cnn,\n            miou_imp_trans,\n            miou_my_cnn,\n            miou_my_cnn_imp,\n        ],\n    }\n)\n\nprint(results)","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["            Model      mIoU\n","0    Baseline-CNN  0.133739\n","1  Baseline-Trans  0.649590\n","2    Improved-CNN  0.257460\n","3  Improved-Trans  0.684180\n","4          My-CNN  0.051553\n","5      My-CNN-Imp  0.054332\n"]}],"execution_count":14},{"id":"484b4664","cell_type":"markdown","source":"Ввыводы\nДобавление механизма SCSE приводит к небольшому приросту mIoU: с 0.052 до 0.054.\n\nНесмотря на улучшение, модель остаётся слишком компактной.\n\nУсиление внимания помогает, но для заметного повышения качества требуется более глубокая или широкая архитектура, либо использование предобученных энкодеров.\n\nВывод\nСегментаторы на базе трансформеров (Segformer) последовательно превосходят CNN-модели. Даже с улучшениями, CNN-архитектурам не хватает мощности — без серьёзного увеличения вычислительных возможностей и глубины они не способны конкурировать с attention-базированными решениями.\n\n","metadata":{}}]}